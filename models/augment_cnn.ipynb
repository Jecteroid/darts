{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook augment_cnn.ipynb to script\n",
      "[NbConvertApp] Writing 3237 bytes to augment_cnn.py\n"
     ]
    }
   ],
   "source": [
    "## please delete below code after convertion in converted script(py) file\n",
    "## + 필요없는 내용 삭제(초반부 1,3~14열, In[ ]형태의 주석제거)\n",
    "!jupyter nbconvert --to script augment_cnn.ipynb\n",
    "!sed -i '/^#[ ]In\\[/d' augment_cnn.py\n",
    "!sed -i -e '1d;3,14d' augment_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from models.augment_cells import AugmentCell\n",
    "from models import ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AuxiliaryHead(nn.Module):\n",
    "    def __init__(self, input_size, C, n_classes):\n",
    "        assert input_size in [7,8]\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(5, stride=input_size-5, padding=0, count_include_pad=False), # 2x2 out\n",
    "            nn.Conv2d(C, 128, kernel_size=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 768, kernel_size=2, bias=False), #1x1 out\n",
    "            nn.BatchNorm2d(768),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.linear = nn.Linear(768, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        out = out.view(out.size(0), -1) #flatten\n",
    "        logits = self.linear(out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentCNN(nn.Module):\n",
    "    def __init__(self, input_size, C_in, C, n_classes, n_layers, auxiliary, genotype, stem_multiplier=3):\n",
    "        super().__init__()\n",
    "        self.C_in = C_in\n",
    "        self.C = C\n",
    "        self.n_classes = n_classes\n",
    "        self.n_layers = n_layers\n",
    "        self.genotype = genotype\n",
    "        \n",
    "        self.aux_pos = 2*n_layers//3 if auxiliary else -1\n",
    "        \n",
    "        C_cur = stem_multiplier * C\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(C_in, C_cur, 3,1,1,bias=False),\n",
    "            nn.BatchNorm2d(C_cur)\n",
    "        )\n",
    "    \n",
    "        C_pp, C_p, C_cur = C_cur, C_cur, C\n",
    "        \n",
    "        self.cells = nn.ModuleList()\n",
    "        reduction_p = False\n",
    "        for i in range(n_layers):\n",
    "            if i in [n_layers//3, 2*n_layers//3]:\n",
    "                C_cur *= 2\n",
    "                reduction = True\n",
    "            else:\n",
    "                reduction = False\n",
    "                \n",
    "            cell = AugmentCell(genotype, C_pp, C_p, C_cur, reduction_p, reduction)\n",
    "            reduction_p = reduction\n",
    "            self.cells.append(cell)\n",
    "            C_cur_out = C_cur * len(cell.concat)\n",
    "            C_pp, C_p = C_p, C_cur_out\n",
    "            \n",
    "            if i == self.aux_pos:\n",
    "                self.aux_head = AuxiliaryHead(input_size//4, C_p, n_classes)\n",
    "        \n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.linear = nn.Linear(C_p, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        s0 = s1 = self.stem(x)\n",
    "        \n",
    "        aux_logits = None\n",
    "        for i, cell in enumerate(self.cells):\n",
    "            s0, s1 = s1, cell(s0, s1)\n",
    "            if i == self.aux_pos and self.training:\n",
    "                aux_logits = self.aux_head(s1)\n",
    "        \n",
    "        out = self.gap(s1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        logits = self.linear(out)\n",
    "        return logits, aux_logits\n",
    "    \n",
    "    def drop_path_prob(self, p):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, ops.DropPath_):\n",
    "                module.p = p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_torch1_cu10",
   "language": "python",
   "name": "py36_torch1_cu10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
