{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook search.ipynb to script\n",
      "[NbConvertApp] Writing 6520 bytes to search.py\n"
     ]
    }
   ],
   "source": [
    "## please delete below code after convertion in converted script(py) file\n",
    "!jupyter nbconvert --to script search.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imported models lib\n"
     ]
    }
   ],
   "source": [
    "import  torch\n",
    "from    torch import nn\n",
    "import  torch.nn.functional as F\n",
    "from    models.operations import OPS, FactorizedReduce, ReLUConvBN\n",
    "from    models.genotypes import PRIMITIVES, Genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, c, stride):\n",
    "        \n",
    "        super(MixedLayer, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for primitive in PRIMITIVES:\n",
    "            layer = OPS[primirive](c, stride, False)\n",
    "        \n",
    "            if 'pool' in primitive:\n",
    "                layer = nn.Sequential(layer, nn.BatchNorm2d(c, affine=False))\n",
    "                \n",
    "            self.layers.append(layer)\n",
    "            \n",
    "    def forward(self, x, weights):\n",
    "        \n",
    "        res = [w * layer(x) for w,layer in zip(weights, self.layers)]\n",
    "        \n",
    "        res = sum(res)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cell(nn.Module):\n",
    "    \n",
    "    def __init__(self, steps, multiplier, cpp, cp, c, reduction, reduction_prev):\n",
    "        super(Cell, self).__init__()\n",
    "        \n",
    "        self.reduction = reduction\n",
    "        self.reduction_prev = reduction_prev\n",
    "        \n",
    "        if reduction_prev:\n",
    "            self.preprocess0 = FactorizedReduce(cpp, c, affine=False)\n",
    "        else :\n",
    "            self.preprocess0 = ReLUConvBN(cpp, c, 1, 1, 0, affine=False)\n",
    "            \n",
    "        self.preprocess1 = ReLUConvBN(cp, c, 1,1,0,affine=False)\n",
    "        \n",
    "        self.steps=steps\n",
    "        self.multiplier = multiplier\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(self.steps):\n",
    "            for j in range(2+i):\n",
    "                stride = 2 if reduction and j < 2 else 1\n",
    "                layer = MixedLayer(c, stride)\n",
    "                self.layers.append(layer)\n",
    "                \n",
    "    def forward(self, s0, s1, weights):\n",
    "        s0 = self.preprocess0(s0)\n",
    "        s1 = self.preprocess1(s1)\n",
    "        \n",
    "        states = [s0, s1]\n",
    "        offset = 0\n",
    "        \n",
    "        for i in range(self.steps):\n",
    "            s = sum(self.layers[offset + j](h, weights[offset+j]) for j, h in enumerate(states))\n",
    "            offset += len(states)\n",
    "            \n",
    "            states.append(s)\n",
    "            \n",
    "        return torch.cat(states[-self.multiplier:], dim=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, c, num_classes, layers, criterion, steps=4, multiplier=4, stem_multiplier=3):\n",
    "        \n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.c = c\n",
    "        self.num_classes = num_classes\n",
    "        self.layers = layers\n",
    "        self.creterion = criterion\n",
    "        self.steps = steps\n",
    "        self.multiplier = multiplier\n",
    "        \n",
    "        c_curr = stem_multiplier *c\n",
    "        \n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, c_curr, 3, padding=1, bias = False),\n",
    "            nn.BatchNorm2d(c_curr)\n",
    "        )\n",
    "        \n",
    "        cpp, cp, c_curr = c_curr, c_curr, c\n",
    "        self.cells = nn.ModuleList()\n",
    "        reduction_prev = False\n",
    "        for i in range(layers):\n",
    "            if i in [layers // 3, 2 * layers // 3]:\n",
    "                c_curr *= 2\n",
    "                reduction= True\n",
    "            else:\n",
    "                reduction= False\n",
    "                \n",
    "            cell = Cell(steps, multiplier, cpp, cp, c_curr, reduction, reduction_prev)\n",
    "            \n",
    "            reduction_prev = reduction\n",
    "            \n",
    "            self.cells += [cell]\n",
    "            \n",
    "            cpp, cp = cp, multiplier * c_curr\n",
    "            \n",
    "        self.global_pooling = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        self.classifier = nn.Linear(cp, num_classes)\n",
    "        \n",
    "        k = sum(1 for i in range(self.steps) for j in range(2+i))\n",
    "        num_ops = len(PRIMITIVES)\n",
    "            \n",
    "        self.alpha_normal = nn.Parameter(torch.randn(k, num_ops))\n",
    "        self.alpha_reduce = nn.parameter(torch.randn(k, num_ops))\n",
    "        with torch.no_grad():\n",
    "            self.alpha_normal.mul_(1e-3)\n",
    "            self.alpha_reduce.mul_(1e-3)\n",
    "        self._arch_parameters = [\n",
    "            self.alpha_normal,\n",
    "            self.alpha_reduce,\n",
    "        ]\n",
    "    \n",
    "    \n",
    "    def new(self):\n",
    "        model_new = Network(self.c, self.num_classes, self.layers, self.criterion).cuda()\n",
    "        for x,y in zip(model_new.arch_parameters(), self.arch_parameters()):\n",
    "            x.data.copy_(y.data)\n",
    "        return model_new\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        s0 = s1 = self.stem(x)\n",
    "        \n",
    "        for i, cell in enumerate(self.sells):\n",
    "            \n",
    "            if cell.reduction:\n",
    "                weights = F.softmax(self.alpha_reduce, dim=-1)\n",
    "            else:\n",
    "                weights = F.softmax(self.alpha_normal, dim=-1)\n",
    "            \n",
    "            s0, s1 = s1, cell(s0, s1, weights)\n",
    "            \n",
    "        out = self.global_pooling(s1)\n",
    "        logits = self.classifier(out.view(out.size(0), -1))\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    \n",
    "    def loss(self, x, target):\n",
    "        logits = self(x)\n",
    "        return self.creterion(logits, target)\n",
    "    \n",
    "    \n",
    "    def arch_parameters(self):\n",
    "        return self._arch_parameters\n",
    "    \n",
    "    \n",
    "    def genotype(self):\n",
    "        \n",
    "        def _parse(weights):\n",
    "            gene = []\n",
    "            n = 2\n",
    "            start = 0\n",
    "            for i in range(self.steps):\n",
    "                end = start+n\n",
    "                W = weights[start:end].copy()\n",
    "                edges = sorted(range(i+2),\n",
    "                              key=lambda x: -max(W[x][k]\n",
    "                                                    for k in range(len(W[x]))\n",
    "                                                    if k != PRIMITIVES.index('none'))\n",
    "                              )[:2]\n",
    "                for j in edges:\n",
    "                    k_best = None\n",
    "                    for k in range(len(W[j])):\n",
    "                        if k != PRIMITIVES.index('none'):\n",
    "                            if k_best is None or W[j][k] > W[j][k_best]:\n",
    "                                k_best = k\n",
    "                    gene.append((PRIMITIVES[k_best],j))\n",
    "                start = end\n",
    "                n += 1\n",
    "            return gene\n",
    "        \n",
    "        gene_normal = _parse(F.softmax(self.alpha_normal, dim=-1).data.cpu().numpy())\n",
    "        gene_reduce = _parse(F.softmax(self.alpht_reduce, dim=-1).data.cpu().numpy())\n",
    "        \n",
    "        concat = range(2+self.steps - self.multiplier, self.steps +2)\n",
    "        genotype = Genotype(\n",
    "            normal = gene_normal, normal_concat=concat,\n",
    "            reduce = gene_reduce, reduce_concat=concat\n",
    "        )\n",
    "        \n",
    "        return genotype"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_torch1_cu10",
   "language": "python",
   "name": "py36_torch1_cu10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
