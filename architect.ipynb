{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook architect.ipynb to script\n",
      "[NbConvertApp] Writing 3436 bytes to architect.py\n"
     ]
    }
   ],
   "source": [
    "## please delete below code after convertion in converted script(py) file\n",
    "## + 필요없는 내용 삭제(초반부 1,3~14열, In[ ]형태의 주석제거)\n",
    "!jupyter nbconvert --to script architect.ipynb\n",
    "!sed -i '/^#[ ]In\\[/d' architect.py\n",
    "!sed -i -e '1d;3,14d' architect.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Architect():\n",
    "    \n",
    "    def __init__(self, net, w_momentum, w_weight_decay):\n",
    "        self.net = net\n",
    "        self.v_net = copy.deepcopy(net)\n",
    "        self.w_momentum = w_momentum\n",
    "        self.w_weight_decay = w_weight_decay\n",
    "        \n",
    "        \n",
    "    def virtual_step(self, trn_X, trn_y, w_lr, w_optim):\n",
    "        \"\"\"\n",
    "        compute unrolled weight w' (virtual step)\n",
    "        \n",
    "        step process:\n",
    "        1) forward\n",
    "        2) calc loss\n",
    "        3) compute gradient (backprop)\n",
    "        4) update gradient\n",
    "        \n",
    "        Args:\n",
    "            w_lr : learning rate for virtual gradient step (same as weight lr)\n",
    "            w_optim : weights optimizer\n",
    "        \"\"\"\n",
    "        \n",
    "        # forward & calc\n",
    "        loss = self.net.loss(trn_X, trn_y)\n",
    "        \n",
    "        # compute gradient\n",
    "        gradients = torch.autograd.grad(loss, self.net.weights())\n",
    "        \n",
    "        with torch.no_grad(): # no affection main model\n",
    "            \n",
    "            for w, vw, g in zip(self.net.weights(), self.v_net.weights(), gradients): # copy weight to virtual model\n",
    "                m=w_optim.state[w].get('momentum_buffer', 0.) * self.w_momentum\n",
    "                vw.copy_(w - w_lr*(m+g+self.w_weight_decay*w))\n",
    "                \n",
    "            for a, va, in zip(self.net.alphas(), self.v_net.alphas()): # copy alpha to virtual model\n",
    "                va.copy_(a)\n",
    "                \n",
    "                \n",
    "    def unrolled_backward(self, trn_X, trn_y, val_X, val_y, w_lr, w_optim):\n",
    "        \n",
    "        self.virtual_step(trn_X, trn_y, w_lr, w_optim)\n",
    "        \n",
    "        loss = self.v_net.loss(val_X, val_y)\n",
    "        \n",
    "        v_alphas = tuple(self.v_net.alphas())\n",
    "        v_weights = tuple(self.v_net.weights())\n",
    "        v_grads = torch.autograd.grad(loss, v_alphas + v_weights)\n",
    "        dalpha = v_grads[:len(v_alphas)] # alpha gradient\n",
    "        dw = v_grads[len(v_alphas):] # weight gradient \n",
    "        \n",
    "        hessian = self.compute_hessian(dw, trn_X, trn_y)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for alpha, da, h in zip(self.net.alphas(), dalpha, hessian): # change alpha's gradient\n",
    "                alpha.grad = da - w_lr * h\n",
    "    \n",
    "    \n",
    "    def compute_hessian(self, dw, trn_X, trn_y):\n",
    "        \"\"\"\n",
    "        dw = dw` { L_val(w`, alpha) }\n",
    "        w+ = w + eps * dw\n",
    "        w- = w - eps * dw\n",
    "        hessian = (dalpha { L_trn(w+, alpha) } - dalpha { L_trn(w-, alpha) }) / (2*eps)\n",
    "        eps = 0.01 / ||dw||\n",
    "        \"\"\"\n",
    "        norm = torch.cat([w.view(-1) for w in dw]).norm()\n",
    "        eps = 0.01 / norm\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for p, d in zip(self.net.weights(), dw):\n",
    "                p+= eps * d\n",
    "        loss = self.net.loss(trn_X, trn_y)\n",
    "        dalpha_pos = torch.autograd.grad(loss, self.net.alphas())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for p, d in zip(self.net.weights(), dw):\n",
    "                p -= 2. * eps * d\n",
    "        loss = self.net.loss(trn_X, trn_y)\n",
    "        dalpha_neg = torch.autograd.grad(loss, self.net.alphas())\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for p,d in zip(self.net.weights(), dw):\n",
    "                p += eps * d\n",
    "                \n",
    "        hessian = [(p-n) / 2.*eps for p,n in zip(dalpha_pos, dalpha_neg)]\n",
    "        return hessian\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_torch1_cu10",
   "language": "python",
   "name": "py36_torch1_cu10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
