{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook search_cnn.ipynb to script\n",
      "[NbConvertApp] Writing 6528 bytes to search_cnn.py\n"
     ]
    }
   ],
   "source": [
    "## please delete below code after convertion in converted script(py) file\n",
    "## + 필요없는 내용 삭제(초반부 1,3~14열, In[ ]형태의 주석제거)\n",
    "!jupyter nbconvert --to script search_cnn.ipynb\n",
    "!sed -i '/^#[ ]In\\[/d' search_cnn.py\n",
    "!sed -i -e '1d;3,14d' search_cnn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from models.search_cells import SearchCell\n",
    "import genotypes as gt\n",
    "from torch.nn.parallel._functions import Broadcast\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def broadcast_list(l, device_ids):\n",
    "    \"\"\"Broadcasting list\"\"\"\n",
    "    l_copies = Broadcast.apply(device_ids, *l)\n",
    "    l_copies = [l_copies[i:i+len(l)] for i in range(0, len(l_copies), len(l))]\n",
    "    \n",
    "    return l_copies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Structure(nn.Module):\n",
    "    \"\"\"CNN model\"\"\"\n",
    "    def __init__(self, C_in, C, n_classes, n_layers, n_nodes=4, stem_multiplier=3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            C_in: # of input channels\n",
    "            C : # of starting model channels\n",
    "            n_classes: # of classes\n",
    "            n_layers: # of layers\n",
    "            n_nodes: # of intermediate nodes in Cell\n",
    "            stem_multiplier\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.C_in = C_in\n",
    "        self.C = C\n",
    "        self.n_classes = n_classes\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        C_cur = stem_multiplier * C\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(C_in, C_cur, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(C_cur)\n",
    "        )\n",
    "        \n",
    "        # in first cell, stem is used for both s0 & s1\n",
    "        # C_pp & C_p is output channel size // C_cur is input channel size\n",
    "        \n",
    "        C_pp, C_p, C_cur = C_cur, C_cur, C\n",
    "        \n",
    "        self.cells = nn.ModuleList()\n",
    "        reduction_p = False\n",
    "        for i in range(n_layers):\n",
    "            # Reduce featuremap size and dougle channels in 1/3 and 2/3 layer.\n",
    "            if i in [n_layers//3, 2*n_layers//3]:\n",
    "                C_cur *= 2\n",
    "                reduction = True\n",
    "            else:\n",
    "                reduction = False\n",
    "                \n",
    "            cell = SearchCell(n_nodes, C_pp, C_p, C_cur, reduction_p, reduction)\n",
    "            reduction_p = reduction\n",
    "            self.cells.append(cell)\n",
    "            C_cur_out = C_cur * n_nodes\n",
    "            C_pp, C_p = C_p, C_cur_out\n",
    "            \n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.linear = nn.Linear(C_p, n_classes)\n",
    "        \n",
    "    def forward(self, x, weights_normal, weights_reduce):\n",
    "        s0 = s1 = self.stem(x)\n",
    "        \n",
    "        for cell in self.cells:\n",
    "            weights = weights_reduce if cell.reduction else weights_normal\n",
    "            s0, s1 = s1, cell(s0, s1, weights)\n",
    "            \n",
    "        out = self.gap(s1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        logits = self.linear(out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchCNNController(nn.Module):\n",
    "    \"\"\"SearchCNN controller supporting multi-gpu\"\"\"\n",
    "    def __init__(self, C_in, C, n_classes, n_layers, criterion, n_nodes=4, stem_multiplier=3, device_ids=None):\n",
    "        super().__init__()\n",
    "        self.n_nodes = n_nodes\n",
    "        self.criterion = criterion\n",
    "        if device_ids is None:\n",
    "            device_ids = list(range(torch.cuda.device_count()))\n",
    "        self.device_ids = device_ids\n",
    "        \n",
    "        # initialize arch parameters: alphas\n",
    "        n_ops = len(gt.PRIMITIVES)\n",
    "        \n",
    "        self.alpha_normal = nn.ParameterList()\n",
    "        self.alpha_reduce = nn.ParameterList()\n",
    "        \n",
    "        for i in range(n_nodes):\n",
    "            self.alpha_normal.append(nn.Parameter(1e-3*torch.randn(i+2, n_ops)))\n",
    "            self.alpha_reduce.append(nn.Parameter(1e-3*torch.randn(i+2, n_ops)))\n",
    "            \n",
    "        # setup alphas list\n",
    "        self._alphas = []\n",
    "        for n, p in self.named_parameters():\n",
    "            if 'alpha' in n:\n",
    "                self._alphas.append((n,p))\n",
    "                \n",
    "        self.net = CNN_Structure(C_in, C, n_classes, n_layers, n_nodes, stem_multiplier)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        weights_normal = [F.softmax(alpha, dim=-1) for alpha in self.alpha_normal]\n",
    "        weights_reduce = [F.softmax(alpha, dim=-1) for alpha in self.alpha_reduce]\n",
    "        \n",
    "        if len(self.device_ids) == 1:\n",
    "            return self.net(x, weights_normal, weights_reduce) # 보통은 여기서 끄읕\n",
    "        \n",
    "        # scatter x\n",
    "        xs = nn.parallel.scatter(x, self.device_ids)\n",
    "        # broadcast weights\n",
    "        wnormal_copies = broadcast_list(weights_normal, self.device_ids)\n",
    "        wreduce_copies = broadcast_list(weights_reduce, self.device_ids)\n",
    "        \n",
    "        # replicate modules\n",
    "        replicas = nn.parallel.replicate(self.net, self.device_ids)\n",
    "        outputs = nn.parallel.parallel_apply(replicas,\n",
    "                                             list(zip(xs, wnormal_copies, wreduce_copies)),\n",
    "                                             devices=self.device_ids)\n",
    "        return nn.parallel.gather(outputs, self.device_ids[0])\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        logits = self.forward(X)\n",
    "        return self.criterion(logits, y)\n",
    "    \n",
    "    def print_alphas(self, logger):\n",
    "        # remove formats\n",
    "        org_formatters = []\n",
    "        for handler in logger.handlers:\n",
    "            org_formatters.append(handler.formatter)\n",
    "            handler.setFormatter(logging.Formatter(\"%(message)s\"))\n",
    "            \n",
    "        logger.info(\"########--Alpha--########\")\n",
    "        logger.info(\"## Alpha - normal\")\n",
    "        for alpha in self.alpha_normal:\n",
    "            logger.info(F.softmax(alpha, dim=-1))\n",
    "            \n",
    "        logger.info(\"\\n## Alpha - reduce\")\n",
    "        for alpha in self.alpha_reduce:\n",
    "            logger.info(F.softmax(alpha, dim=-1))\n",
    "            \n",
    "        logger.info(\"#########################\")\n",
    "        \n",
    "        # restore formats\n",
    "        for handler, formatter in zip(logger.handlers, org_formatters):\n",
    "            handler.setFormatter(formatter)\n",
    "            \n",
    "    def genotype(self):\n",
    "        gene_normal = gt.parse(self.alpha_normal, k=2)\n",
    "        gene_reduce = gt.parse(self.alpha_reduce, k=2)\n",
    "        concat = range(2, 2+self.n_nodes) # concat all intermediate nodes\n",
    "        \n",
    "        return gt.Genotype(normal=gene_normal, normal_concat=concat,\n",
    "                           reduce=gene_reduce, reduce_concat=concat)\n",
    "    \n",
    "    def weights(self):\n",
    "        return self.net.parameters()\n",
    "    \n",
    "    def named_weights(self):\n",
    "        return self.net.named_parameters()\n",
    "    \n",
    "    def alphas(self):\n",
    "        for n, p in self._alphas:\n",
    "            yield p\n",
    "            \n",
    "    def named_alphas(self):\n",
    "        for n, p in self._alphas:\n",
    "            yield n, p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_torch1_cu10",
   "language": "python",
   "name": "py36_torch1_cu10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
