{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook preproc.ipynb to script\n",
      "[NbConvertApp] Writing 476 bytes to preproc.py\n"
     ]
    }
   ],
   "source": [
    "## please delete below code after convertion in converted script(py) file\n",
    "## + 필요없는 내용 삭제(초반부 1,3~14열, In[ ]형태의 주석제거)\n",
    "!jupyter nbconvert --to script preproc.ipynb\n",
    "!sed -i '/^#[ ]In\\[/d' preproc.py\n",
    "!sed -i -e '1d;3,14d' preproc.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cutout(object):\n",
    "    def __init__(self, length):\n",
    "        self.length = length\n",
    "\n",
    "    def __call__(self, img):\n",
    "        h, w = img.size(1), img.size(2)\n",
    "        mask = np.ones((h, w), np.float32)\n",
    "        y = np.random.randint(h)\n",
    "        x = np.random.randint(w)\n",
    "\n",
    "        y1 = np.clip(y - self.length // 2, 0, h)\n",
    "        y2 = np.clip(y + self.length // 2, 0, h)\n",
    "        x1 = np.clip(x - self.length // 2, 0, w)\n",
    "        x2 = np.clip(x + self.length // 2, 0, w)\n",
    "\n",
    "        mask[y1: y2, x1: x2] = 0.\n",
    "        mask = torch.from_numpy(mask)\n",
    "        mask = mask.expand_as(img)\n",
    "        img *= mask\n",
    "\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_transforms(dataset, cutout_length):\n",
    "    dataset = dataset.lower()\n",
    "    if dataset == 'cifar10':\n",
    "        MEAN = [0.49139968, 0.48215827, 0.44653124]\n",
    "        STD = [0.24703233, 0.24348505, 0.26158768]\n",
    "        transf = [\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip()\n",
    "        ]\n",
    "    elif dataset == 'mnist':\n",
    "        MEAN = [0.13066051707548254]\n",
    "        STD = [0.30810780244715075]\n",
    "        transf = [\n",
    "            transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=0.1)\n",
    "        ]\n",
    "    elif dataset == 'fashionmnist':\n",
    "        MEAN = [0.28604063146254594]\n",
    "        STD = [0.35302426207299326]\n",
    "        transf = [\n",
    "            transforms.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=0.1),\n",
    "            transforms.RandomVerticalFlip()\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError('not expected dataset = {}'.format(dataset))\n",
    "\n",
    "    normalize = [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(MEAN, STD)\n",
    "    ]\n",
    "\n",
    "    train_transform = transforms.Compose(transf + normalize)\n",
    "    valid_transform = transforms.Compose(normalize)\n",
    "\n",
    "    if cutout_length > 0:\n",
    "        train_transform.transforms.append(Cutout(cutout_length))\n",
    "\n",
    "    return train_transform, valid_transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36_torch1_cu10",
   "language": "python",
   "name": "py36_torch1_cu10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
